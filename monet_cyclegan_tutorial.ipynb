{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Requirements\n##Installs\ndef check_dependencies_already_installed() -> bool:\n    try:\n        import tensorflow_addons\n        return True\n    except ImportError:\n        return False\n\n\n\ndef install_dependencies():\n    !pip install 'tensorflow_addons' 'tensorflow-determinism' 'gdown'\n\n\ndependencies_already_installed = check_dependencies_already_installed()\nprint(f'dependencies_already_installed: {dependencies_already_installed}')\nif not dependencies_already_installed:\n    install_dependencies()\n\n\n##Imports\nimport os\nimport random\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.python.data.ops.dataset_ops import Dataset\nfrom tensorflow.python.distribute.tpu_strategy import TPUStrategy\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom pathlib import Path\nfrom datetime import datetime\nfrom zipfile import ZipFile\n\n#Environment setup\n##Set tensorflow deterministic mode\ndef set_tf_deterministic_mode():\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n\n\n##Set random seed\ndef set_training_random_seed(seed: int):\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print(f'set_training_random_seed() - seed value: {seed}')\n\n\n##Connect to strongest available device\nset_tf_deterministic_mode()\n\ndef choose_strongest_available_device_strategy():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        selected_strategy = TPUStrategy(tpu)\n    except:\n        selected_strategy = tf.distribute.get_strategy()\n\n    print(f\"choose_strongest_available_device_strategy() - selected strategy type: {type(selected_strategy).__name__}\")\n\n    # todo itay - delete this section so it won't mess up in google colab\n    gpu_is_available = any(tf.config.list_physical_devices('GPU'))\n    if gpu_is_available:\n        !nvidia-smi\n\n    return selected_strategy\n\nDEVICE_STRATEGY = choose_strongest_available_device_strategy()\n\n# todo itay - delete\nassert tf.__version__ == '2.6.4'\n\n##Download competition dataset\nLOCAL_DATASET_FOLDER_PATH = Path('./train_data/gan-getting-started/')\n\ndef download_competition_dataset_if_not_present():\n    dataset_already_downloaded = LOCAL_DATASET_FOLDER_PATH.exists()\n    print(f\"dataset_already_downloaded: {dataset_already_downloaded}\")\n    if not dataset_already_downloaded:\n        # note - this is the untouched competition dataset. just uploaded it to the drive so it'll be\n        # available via colab as well.\n        !gdown 1ZwcoO11NKhYsbuM7hzdSzKjGOnOx6X94\n        !mkdir -p {LOCAL_DATASET_FOLDER_PATH}\n        !unzip -o -q ./gan-getting-started.zip -d {LOCAL_DATASET_FOLDER_PATH}\n\ndownload_competition_dataset_if_not_present()\n\n\ndef _choose_30_images(\n        original_monet_dataset: Dataset, choose_30_images_strategy: dict, experiment_random_seed: int\n) -> Dataset:\n    method = choose_30_images_strategy['method']\n    selection_random_seed = choose_30_images_strategy['random_seed']\n    method_func_params = choose_30_images_strategy.get('params', None)\n\n    np.random.seed(selection_random_seed)\n    try:\n        if method == 'random_selection':\n            chosen_30_monet_dataset = _pick_random_images(\n                original_monet_ds=original_monet_dataset,\n                images_count=30\n            )\n        elif method == 'pick_images_farthest_from_each_other':\n            chosen_30_monet_dataset = _pick_images_farthest_from_each_other(\n                original_monet_ds=original_monet_dataset,\n                images_count=30\n            )\n        else:\n            raise NotImplementedError(f\"unknown method - '{method}'\")\n\n        images_shape = list(chosen_30_monet_dataset)[0].shape\n        print(f'*** Selected 30 train monet photos (shape: {images_shape}) ***')\n        _, ax = plt.subplots(30, 1, figsize=(50, 50))\n        for i, img in enumerate(chosen_30_monet_dataset):\n            img = (img * 127.5 + 127.5).numpy()[0].astype(np.uint8)\n\n            ax[i].imshow(img)\n        plt.show()\n\n    finally:\n        np.random.seed(experiment_random_seed)\n    return chosen_30_monet_dataset\n\n\n\n#Load competition dataset\n##Load full dataset\ndef find_competition_dataset_files(local_dataset_folder_path: Path):\n    if isinstance(DEVICE_STRATEGY, TPUStrategy):\n        from kaggle_datasets import KaggleDatasets\n        dataset_folder_path = Path(KaggleDatasets().get_gcs_path())\n    else:\n        dataset_folder_path = local_dataset_folder_path\n\n    monet_dataset_files = tf.io.gfile.glob(str(dataset_folder_path / 'monet_tfrec/*.tfrec'))\n    photo_dataset_files = tf.io.gfile.glob(str(dataset_folder_path / 'photo_tfrec/*.tfrec'))\n    assert any(monet_dataset_files)\n    assert any(photo_dataset_files)\n    print(f\"found {len(monet_dataset_files)} monet and {len(photo_dataset_files)} photo tfrec files.\")\n\n    return monet_dataset_files, photo_dataset_files\n\n\ndef load_tf_records_dataset(tf_record_files) -> Dataset:\n    def _read_and_normalize_tfrecord(record):\n        tfrecord_format = {\n            \"image_name\": tf.io.FixedLenFeature([], tf.string),\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            \"target\": tf.io.FixedLenFeature([], tf.string)\n        }\n        record = tf.io.parse_single_example(record, tfrecord_format)\n        image = record['image']\n        image = tf.image.decode_jpeg(image, channels=3)\n        # print(f'asdsadsad: {(image.shape, type(image))}')\n        # image = tf.keras.utils.array_to_img(image)\n        # image = image.resize((320, 320))\n        # image = np.array(image)\n        # image = tf.convert_to_tensor(image)\n        image = (tf.cast(image, tf.float32) / 127.5) - 1\n        image = tf.reshape(image, [256, 256, 3])\n        image = tf.image.resize(image, (320, 320), method='bilinear')\n        return image\n\n    sorted_tf_record_files = sorted(tf_record_files)\n    dataset = tf.data.TFRecordDataset(sorted_tf_record_files)\n    dataset = dataset.map(_read_and_normalize_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    return dataset\n\n\n##Pick 30 train monet images strategies\ndef _pick_images_farthest_from_each_other(original_monet_ds: Dataset, images_count: int) -> Dataset:\n    def _images_distance(im1, im2):\n        distance = np.sum((im1.numpy().flatten() - im2.numpy().flatten()) ** 2)\n        return distance\n\n    farthest_images_list = _incremental_farthest_search(\n        list(original_monet_ds), k=images_count, distance_func=_images_distance\n    )\n    farthest_images_dataset = Dataset.from_tensor_slices(farthest_images_list)\n    return farthest_images_dataset\n\n\ndef _pick_random_images(original_monet_ds: Dataset, images_count: int) -> Dataset:\n    monet_images_list = list(original_monet_ds)\n    ret_dataset = Dataset.from_tensor_slices([\n        monet_images_list[image_idx] for image_idx in\n        np.random.choice(\n            list(range(len(monet_images_list))), size=images_count, replace=False\n        )\n    ])\n    return ret_dataset\n\n\ndef _incremental_farthest_search(array, k: int, distance_func):\n    remaining_points = array[:]\n    solution_set = [remaining_points.pop(random.randint(0, len(remaining_points) - 1))]\n\n    for _ in tqdm(list(range(k - 1)), desc='incremental_farthest_search() main loop'):\n        distances = [distance_func(p, solution_set[0]) for p in remaining_points]\n        for i, p in enumerate(remaining_points):\n            for j, s in enumerate(solution_set):\n                distances[i] = min(distances[i], distance_func(p, s))\n        solution_set.append(remaining_points.pop(distances.index(max(distances))))\n    return solution_set\n\n\ndef down_sample(filters, size, strides=2, padding='same'):\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n\n    network = keras.Sequential()\n    network.add(layers.Conv2D(filters, size, strides=strides, padding=padding,\n                             kernel_initializer=initializer, use_bias=False))\n\n    network.add(layers.LeakyReLU())\n\n    return network\n\n\ndef up_sample(filters, size, strides=2, padding='same', apply_dropout=False):\n    network = keras.Sequential()\n    network.add(layers.Conv2DTranspose(\n        filters, size, strides=strides, padding=padding, use_bias=False,\n        kernel_initializer=tf.random_normal_initializer(0., 0.02)\n    ))\n    if apply_dropout:\n        network.add(layers.Dropout(0.5))\n    network.add(layers.ReLU())\n    return network\n\n\ndef build_generator_model():\n    inputs = layers.Input(shape=[320, 320, 3])\n\n    # bs = batch size\n    down_stack = [\n        down_sample(64, 4), # (bs, 160, 160, 64)\n        down_sample(128, 4), # (bs, 80, 80, 128)\n        down_sample(256, 4), # (bs, 40, 40, 256)\n        down_sample(512, 4), # (bs, 20, 20, 512)\n        down_sample(512, 4), # (bs, 10, 10, 512)\n        down_sample(512, 4), # (bs, 5, 5, 512)\n        down_sample(512, 4, strides=1, padding='valid'), # (bs, 2, 2, 512)\n        down_sample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        up_sample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        up_sample(512, 4, strides=1, padding='valid', apply_dropout=True), # (bs, 5, 5, 1024)\n        up_sample(512, 4, apply_dropout=True), # (bs, 10, 10, 1024)\n        up_sample(512, 4), # (bs, 20, 20, 1024)\n        up_sample(256, 4), # (bs, 40, 40, 512)\n        up_sample(128, 4), # (bs, 80, 80, 256)\n        up_sample(64, 4), # (bs, 160, 160, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(3, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 320, 320, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    generator_model = keras.Model(inputs=inputs, outputs=x)\n    return generator_model\n\n\"\"\"# Build the discriminator\n\nThe discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification.\n\"\"\"\n\ndef build_discriminator_model():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inp = layers.Input(shape=[320, 320, 3], name='input_image')\n\n    x = inp\n    down1 = down_sample(64, 5)(x) # (bs, 160, 160, 64)\n    down2 = down_sample(128, 4)(down1) # (bs, 80, 80, 128)\n    down3 = down_sample(256, 3)(down2) # (bs, 40, 40, 256)\n    down4 = down_sample(256, 2)(down3) # (bs, 20, 20, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down4) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    leaky_relu = layers.LeakyReLU()(conv)\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    discriminator_model = tf.keras.Model(inputs=inp, outputs=last)\n    return discriminator_model\n\n\nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n\n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n\n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }\n\n\"\"\"# Define loss functions\n\nThe discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss.\n\"\"\"\n\n\"\"\"# Create submission file\"\"\"\nCREATE_PREDICTIONS = True\n\ndef create_predictions_for_kaggle_submission(monet_generator: tf.keras.Model, photo_dataset: Dataset):\n    wip_images_folder = Path(f\"/tmp/wip_{datetime.now().strftime('%y_%m_%d__%H_%M_%S')}/\")\n    wip_images_folder.mkdir(parents=True, exist_ok=True)\n    output_zip_path = Path('/kaggle/working/images.zip')\n    if output_zip_path.exists():\n        output_zip_path.unlink()\n\n    with ZipFile(output_zip_path, 'w') as output_images_zip, \\\n        tqdm(total=7_038, desc='generating prediction images for kaggle submission') as pbar:\n        for i, img in enumerate(photo_dataset):\n            prediction = monet_generator(img, training=False)[0]\n            prediction = tf.image.resize(prediction, (256, 256), method='bilinear')\n            prediction = prediction.numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            im = Image.fromarray(prediction)\n            wip_image_file_path = wip_images_folder / f'{i + 1}.jpg'\n            im.save(wip_image_file_path)\n            output_images_zip.write(filename=wip_image_file_path, arcname=wip_image_file_path.name)\n            wip_image_file_path.unlink()\n            pbar.update()\n\n\ndef experiment_flow(\n        choose_30_images_strategy: dict,\n        train_settings: dict,\n        experiment_random_seed: int,\n        create_kaggle_predictions_for_submission: bool = False,\n):\n    set_training_random_seed(experiment_random_seed)\n\n    monet_dataset_files, photo_dataset_files = find_competition_dataset_files(LOCAL_DATASET_FOLDER_PATH)\n    original_monet_dataset = load_tf_records_dataset(monet_dataset_files).batch(1)\n    photo_dataset = load_tf_records_dataset(photo_dataset_files).batch(1)\n\n    chosen_30_monet_dataset = _choose_30_images(original_monet_dataset, choose_30_images_strategy, experiment_random_seed)\n\n    with DEVICE_STRATEGY.scope():\n        monet_generator = build_generator_model() # transforms photos to Monet-esque paintings\n        photo_generator = build_generator_model() # transforms Monet paintings to be more like photos\n\n        monet_discriminator = build_discriminator_model() # differentiates real Monet paintings and generated Monet paintings\n        photo_discriminator = build_discriminator_model() # differentiates real photos and generated photos\n\n    with DEVICE_STRATEGY.scope():\n        def discriminator_loss(real, generated):\n            real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n            generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n            total_disc_loss = real_loss + generated_loss\n\n            return total_disc_loss * 0.5\n\n    \"\"\"The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss.\"\"\"\n\n    with DEVICE_STRATEGY.scope():\n        def generator_loss(generated):\n            return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n\n    \"\"\"We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference.\"\"\"\n\n    with DEVICE_STRATEGY.scope():\n        def calc_cycle_loss(real_image, cycled_image, lambda_):\n            loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n            return lambda_ * loss1\n\n    \"\"\"The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator.\"\"\"\n\n    with DEVICE_STRATEGY.scope():\n        def identity_loss(real_image, same_image, lambda_):\n            loss = tf.reduce_mean(tf.abs(real_image - same_image))\n            return lambda_ * 0.5 * loss\n\n    \"\"\"# Train the CycleGAN\n    \n    Let's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model.\n    \"\"\"\n\n    with DEVICE_STRATEGY.scope():\n        optimizer_builder = train_settings['optimizer_builder']\n        monet_generator_optimizer = optimizer_builder()\n        photo_generator_optimizer = optimizer_builder()\n\n        monet_discriminator_optimizer = optimizer_builder()\n        photo_discriminator_optimizer = optimizer_builder()\n\n    with DEVICE_STRATEGY.scope():\n        cycle_gan_model = CycleGan(\n            monet_generator, photo_generator, monet_discriminator, photo_discriminator\n        )\n\n        cycle_gan_model.compile(\n            m_gen_optimizer = monet_generator_optimizer,\n            p_gen_optimizer = photo_generator_optimizer,\n            m_disc_optimizer = monet_discriminator_optimizer,\n            p_disc_optimizer = photo_discriminator_optimizer,\n            gen_loss_fn = generator_loss,\n            disc_loss_fn = discriminator_loss,\n            cycle_loss_fn = calc_cycle_loss,\n            identity_loss_fn = identity_loss\n        )\n\n    print(f'\\n\\n*** running_on_tpu - {isinstance(DEVICE_STRATEGY, TPUStrategy)} ***\\n\\n')\n\n    cycle_gan_model.fit(\n        tf.data.Dataset.zip((chosen_30_monet_dataset, photo_dataset)),\n        epochs=train_settings['train_epochs']\n    )\n\n    print('*** Show trained model predictions sample ***')\n    _, ax = plt.subplots(5, 2, figsize=(12, 12))\n    for i, img in enumerate(photo_dataset.take(5)):\n        prediction = monet_generator(img, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n        ax[i, 0].imshow(img)\n        ax[i, 1].imshow(prediction)\n        ax[i, 0].set_title(\"Input Photo\")\n        ax[i, 1].set_title(\"Monet-esque\")\n        ax[i, 0].axis(\"off\")\n        ax[i, 1].axis(\"off\")\n    plt.show()\n\n    if create_kaggle_predictions_for_submission:\n        create_predictions_for_kaggle_submission(monet_generator, photo_dataset)\n\n\nexperiment_flow(\n    choose_30_images_strategy=dict(\n        method='random_selection',\n        random_seed=42\n    ),\n    train_settings=dict(\n        train_epochs=40,\n        optimizer_builder=lambda: tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.001)\n    ),\n    experiment_random_seed=1,\n    create_kaggle_predictions_for_submission=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-14T13:00:27.122461Z","iopub.execute_input":"2023-02-14T13:00:27.122851Z","iopub.status.idle":"2023-02-14T13:00:28.483047Z","shell.execute_reply.started":"2023-02-14T13:00:27.122814Z","shell.execute_reply":"2023-02-14T13:00:28.481763Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"dependencies_already_installed: True\nchoose_strongest_available_device_strategy() - selected strategy type: _DefaultDistributionStrategy\nTue Feb 14 13:00:28 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   27C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\ndataset_already_downloaded: True\n","output_type":"stream"}]}]}